# Tracks and renders pose + hands + face landmarks.

# GPU buffer. (GpuBuffer)
input_stream: "input_video"

# GPU image with rendered results. (GpuBuffer)
output_stream: "output_video"

# Sign classification list
output_stream: "classifications"

# Throttles the images flowing downstream for flow control. It passes through
# the very first incoming image unaltered, and waits for downstream nodes
# (calculators and subgraphs) in the graph to finish their tasks before it
# passes through another image. All images that come in while waiting are
# dropped, limiting the number of in-flight images in most part of the graph to
# 1. This prevents the downstream nodes from queuing up incoming images and data
# excessively, which leads to increased latency and memory usage, unwanted in
# real-time mobile applications. It also eliminates unnecessarily computation,
# e.g., the output produced by a node may get dropped downstream if the
# subsequent nodes are still busy processing previous inputs.
node {
  calculator: "FlowLimiterCalculator"
  input_stream: "input_video"
  input_stream: "FINISHED:output_video"
  input_stream_info: {
    tag_index: "FINISHED"
    back_edge: true
  }
  output_stream: "throttled_input_video"
  node_options: {
    [type.googleapis.com/mediapipe.FlowLimiterCalculatorOptions] {
      max_in_flight: 1
      max_in_queue: 1
      # Timeout is disabled (set to 0) as first frame processing can take more
      # than 1 second.
      in_flight_timeout: 0
    }
  }
}

node {
  calculator: "HolisticLandmarkGpu"
  input_stream: "IMAGE:throttled_input_video"
  output_stream: "POSE_LANDMARKS:pose_landmarks"
  output_stream: "POSE_ROI:pose_roi"
  output_stream: "POSE_DETECTION:pose_detection"
  output_stream: "FACE_LANDMARKS:face_landmarks"
  output_stream: "LEFT_HAND_LANDMARKS:left_hand_landmarks"
  output_stream: "RIGHT_HAND_LANDMARKS:right_hand_landmarks"
}

# Gets image size.
node {
  calculator: "ImagePropertiesCalculator"
  input_stream: "IMAGE_GPU:throttled_input_video"
  output_stream: "SIZE:image_size"
}

# # Convert landmarksk to matrix
# node {
#     calculator: "LandmarksToMatrixCalculator"
#     input_stream: "LANDMARKS:pose_landmarks"
#     input_stream: "IMAGE_SIZE:image_size"
#     output_stream: "LANDMARKS_MATRIX:pose_matrix"
# }

# Converts pose, hand and face landmarks to sign language model input
node {
    calculator: "PrepareSignLanguageModelInputCalculator"
    input_stream: "IMAGE_SIZE:image_size"
    input_stream: "POSE_LANDMARKS:pose_landmarks"
    input_stream: "LEFT_HAND_LANDMARKS:left_hand_landmarks"
    input_stream: "RIGHT_HAND_LANDMARKS:right_hand_landmarks"
    input_stream: "FACE_LANDMARKS:face_landmarks"
    output_stream: "SIGN_LANGUAGE_MATRIX:sign_language_matrix"
}


# Converts matrix to tensor
node {
  calculator: "TfLiteConverterCalculator"
  input_stream: "MATRIX:sign_language_matrix"
  output_stream: "TENSORS:sign_language_tensor"
}

  # node_options: {
  #   [type.googleapis.com/mediapipe.TfLiteConverterCalculatorOptions] {
  #     zero_center: false
  #     max_num_channels: 3
  #   }
  #  }

# Runs a TensorFlow Lite model on CPU that takes an image tensor and outputs a
# vector of tensors representing, for instance, detection boxes/keypoints and
# scores.
node {
  calculator: "TfLiteInferenceCalculator"
  input_stream: "TENSORS:sign_language_tensor"
  output_stream: "TENSORS:classification_tensors"
  node_options: {
    [type.googleapis.com/mediapipe.TfLiteInferenceCalculatorOptions] {
      model_path: "mediapipe/models/sign_language.tflite"
    }
  }
}

node {
  calculator: "TfLiteTensorsToClassificationCalculator"
  input_stream: "TENSORS:classification_tensors"
  output_stream: "CLASSIFICATIONS:classifications"
  options: {
    [mediapipe.TfLiteTensorsToClassificationCalculatorOptions.ext] {
      min_score_threshold: 0.5
      label_map_path: "mediapipe/models/sign_language_labels.txt"
    }
  }
}

# Converts Tensor to classification
# node {
#   calculator: "TensorsToClassificationCalculator"
#   input_stream: "TENSORS:classification_tensors"
#   output_stream: "CLASSIFICATIONS:classifications"
#   options: {
#     [mediapipe.TensorsToClassificationCalculatorOptions.ext] {
#       min_score_threshold: 0.5
#       label_map_path: "mediapipe/models/sign_language_labels.txt"
#     }
#   }
# }

# Converts pose, hands and face landmarks to a render data vector.
node {
  calculator: "HolisticTrackingToRenderData"
  input_stream: "IMAGE_SIZE:image_size"
  input_stream: "POSE_LANDMARKS:pose_landmarks"
  input_stream: "POSE_ROI:pose_roi"
  input_stream: "LEFT_HAND_LANDMARKS:left_hand_landmarks"
  input_stream: "RIGHT_HAND_LANDMARKS:right_hand_landmarks"
  input_stream: "FACE_LANDMARKS:face_landmarks"
  output_stream: "RENDER_DATA_VECTOR:render_data_vector"
}

# Draws annotations and overlays them on top of the input images.
node {
  calculator: "AnnotationOverlayCalculator"
  input_stream: "IMAGE_GPU:throttled_input_video"
  input_stream: "VECTOR:render_data_vector"
  output_stream: "IMAGE_GPU:output_video"
}
